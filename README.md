# Tittle
Continual Learning in Autonomous Agents (A Deep Reinforcement Learning project)

# Summary
**Catastrophic Forgetting (CF)** is an intrinsic problem associated with learning through deep neural networks (DNN). It refers to the tendency of these networks to forget previously learned information when trained on new tasks, and it becomes especially relevant in the context of sequential task learning.

This is a major challenge in the development of artificial intelligence (AI) systems that need to learn and adapt to new information over time. It is inherently related to **Continual Learning (CL)** and the pursuit of **Artificial General Intelligence (AGI).**

This problem has been widely documented in supervised learning contexts and has proven to be a significant challenge in preserving and adapting acquired skills. In the field of **Reinforcement Learning (RL)**, where an agent seeks to maximize cumulative reward through interactions with an environment, catastrophic forgetting presents even more complex and less understood implications, largely due to the inherently dynamic and ever-evolving nature of typical environments in this area.

This master's thesis explores the issue of CL in **Deep Learning (DL)**, with special attention to its impact in the field of **Deep Reinforcement Learning (DRL)**. In general, this work aims to contribute both to the understanding of the mechanisms behind catastrophic forgetting and to the various approaches intended to mitigate its effects in deep reinforcement learning, providing a detailed review of the current state of mitigation strategies. Drawing inspiration from proposed solutions in supervised learning and the unique challenges of RL, new approaches are proposed that adapt and combine previous strategies to address this phenomenon.

This work is expected to contribute to a broader understanding of catastrophic forgetting in the RL domain and to the identification of different paths for its mitigation, with the goal of achieving more robust and adaptive DRL systems in sequential learning.

**Keywords**: Catastrophic Forgetting, Deep Reinforcement Learning, Continual Learning, Knowledge Distillation, Meta-Learning, Synaptic Consolidation
